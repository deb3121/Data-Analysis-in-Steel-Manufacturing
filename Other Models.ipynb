{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ccc98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\debji\\anaconda3\\lib\\site-packages (1.7.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\debji\\anaconda3\\lib\\site-packages (from xgboost) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\debji\\anaconda3\\lib\\site-packages (from xgboost) (1.10.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aa5200c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000268 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1105\n",
      "[LightGBM] [Info] Number of data points in the train set: 4874, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 0.007541\n",
      "Random Forest Regressor:\n",
      "RMSE: 0.0022, R2: 0.3851, Hit Rate: 85.81%\n",
      "\n",
      "Support Vector Regressor (SVR):\n",
      "RMSE: 0.0057, R2: -3.2803, Hit Rate: 21.99%\n",
      "\n",
      "XGBoost Regressor:\n",
      "RMSE: 0.0022, R2: 0.3871, Hit Rate: 85.73%\n",
      "\n",
      "LightGBM Regressor:\n",
      "RMSE: 0.0021, R2: 0.4245, Hit Rate: 86.96%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv('DSDataLastThreeMonths.csv')\n",
    "\n",
    "# Select relevant columns for the model\n",
    "selected_columns = ['HM_WT', 'AIM_S', 'HM_S', 'HM_C', 'HM_SI', 'HM_TI', 'HM_MN', 'CAC2', 'MG', 'HM_TEMP', 'CAC2_INJ_TIME', 'MG_INJ_TIME', 'DS_S']\n",
    "\n",
    "dataset = dataset[selected_columns]\n",
    "\n",
    "# Drop rows with missing values in both X and y\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "# Split the dataset into input features (X) and target variable (y)\n",
    "X = dataset.drop('DS_S', axis=1)\n",
    "y = dataset['DS_S']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the Random Forest Regressor model\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Create and train the Support Vector Regressor (SVR) model\n",
    "svr_model = SVR()\n",
    "svr_model.fit(X_train, y_train)\n",
    "svr_y_pred = svr_model.predict(X_test)\n",
    "\n",
    "# Create and train the XGBoost Regressor model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Create and train the LightGBM Regressor model\n",
    "lgb_model = lgb.LGBMRegressor()\n",
    "lgb_model.fit(X_train, y_train)\n",
    "lgb_y_pred = lgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance using RMSE and R2 score\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return rmse, r2\n",
    "\n",
    "# Evaluate the models\n",
    "rf_rmse, rf_r2 = evaluate_model(y_test, rf_y_pred)\n",
    "svr_rmse, svr_r2 = evaluate_model(y_test, svr_y_pred)\n",
    "xgb_rmse, xgb_r2 = evaluate_model(y_test, xgb_y_pred)\n",
    "lgb_rmse, lgb_r2 = evaluate_model(y_test, lgb_y_pred)\n",
    "\n",
    "# Calculate the Model hit rate for each model (% data point with (Pred DS_S â€“ Act DS_S) between +- 0.003%)\n",
    "tolerance = 0.003\n",
    "rf_within_tolerance = abs(rf_y_pred - y_test) <= tolerance\n",
    "rf_hit_rate = (rf_within_tolerance.sum() / len(y_test)) * 100\n",
    "\n",
    "svr_within_tolerance = abs(svr_y_pred - y_test) <= tolerance\n",
    "svr_hit_rate = (svr_within_tolerance.sum() / len(y_test)) * 100\n",
    "\n",
    "xgb_within_tolerance = abs(xgb_y_pred - y_test) <= tolerance\n",
    "xgb_hit_rate = (xgb_within_tolerance.sum() / len(y_test)) * 100\n",
    "\n",
    "lgb_within_tolerance = abs(lgb_y_pred - y_test) <= tolerance\n",
    "lgb_hit_rate = (lgb_within_tolerance.sum() / len(y_test)) * 100\n",
    "\n",
    "# Share summary of the models' performance\n",
    "print(\"Random Forest Regressor:\")\n",
    "print(f\"RMSE: {rf_rmse:.4f}, R2: {rf_r2:.4f}, Hit Rate: {rf_hit_rate:.2f}%\")\n",
    "\n",
    "print(\"\\nSupport Vector Regressor (SVR):\")\n",
    "print(f\"RMSE: {svr_rmse:.4f}, R2: {svr_r2:.4f}, Hit Rate: {svr_hit_rate:.2f}%\")\n",
    "\n",
    "print(\"\\nXGBoost Regressor:\")\n",
    "print(f\"RMSE: {xgb_rmse:.4f}, R2: {xgb_r2:.4f}, Hit Rate: {xgb_hit_rate:.2f}%\")\n",
    "\n",
    "print(\"\\nLightGBM Regressor:\")\n",
    "print(f\"RMSE: {lgb_rmse:.4f}, R2: {lgb_r2:.4f}, Hit Rate: {lgb_hit_rate:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8947a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
